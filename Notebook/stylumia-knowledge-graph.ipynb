{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-28T17:57:15.871524Z",
     "iopub.status.busy": "2024-12-28T17:57:15.871180Z",
     "iopub.status.idle": "2024-12-28T17:57:16.465400Z",
     "shell.execute_reply": "2024-12-28T17:57:16.463681Z",
     "shell.execute_reply.started": "2024-12-28T17:57:15.871496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-05T05:51:25.616941Z",
     "iopub.status.busy": "2025-01-05T05:51:25.616687Z",
     "iopub.status.idle": "2025-01-05T05:54:13.493208Z",
     "shell.execute_reply": "2025-01-05T05:54:13.492282Z",
     "shell.execute_reply.started": "2025-01-05T05:51:25.616918Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.17 requires torch<2.5,>=1.10, but you have torch 2.5.1 which is incompatible.\n",
      "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\n",
      "torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet -qU langchain-huggingface torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T10:01:19.533459Z",
     "iopub.status.busy": "2025-01-05T10:01:19.533135Z",
     "iopub.status.idle": "2025-01-05T10:01:32.469686Z",
     "shell.execute_reply": "2025-01-05T10:01:32.468811Z",
     "shell.execute_reply.started": "2025-01-05T10:01:19.533434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.7/301.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet langchain neo4j langchain-neo4j openai sentence-transformers python-dotenv langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load all the CSV files into separate DataFrames\n",
    "bathroom = pd.read_csv('/kaggle/input/stylumia-detail/Bathroom Vanities Data Dump.csv')\n",
    "kurtis = pd.read_csv('/kaggle/input/stylumia-detail/Data Dump Kurtis.csv')\n",
    "dresses = pd.read_csv('/kaggle/input/stylumia-detail/Dresses Data Dump.csv')\n",
    "earrings = pd.read_csv('/kaggle/input/stylumia-detail/Earrings Data Dump.csv')\n",
    "jeans = pd.read_csv('/kaggle/input/stylumia-detail/Jeans Data Dump.csv')\n",
    "saree = pd.read_csv('/kaggle/input/stylumia-detail/Saree Data Dump.csv')\n",
    "sneakers = pd.read_csv('/kaggle/input/stylumia-detail/Sneakers Data Dump.csv')\n",
    "tshirts = pd.read_csv('/kaggle/input/stylumia-detail/Tshirts Data Dump.csv')\n",
    "watches = pd.read_csv('/kaggle/input/stylumia-detail/Watches Data Dump.csv')\n",
    "shirts = pd.read_csv('/kaggle/input/stylumia-detail/shirts_data_dump.csv')\n",
    "\n",
    "# List of DataFrames and corresponding names\n",
    "dataframes = [\n",
    "    (\"Bathroom\", bathroom),\n",
    "    (\"Kurtis\", kurtis),\n",
    "    (\"Dresses\", dresses),\n",
    "    (\"Earrings\", earrings),\n",
    "    (\"Jeans\", jeans),\n",
    "    (\"Saree\", saree),\n",
    "    (\"Sneakers\", sneakers),\n",
    "    (\"Tshirts\", tshirts),\n",
    "    (\"Watches\", watches),\n",
    "    (\"Shirts\", shirts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine sampled data into a single DataFrame\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Process each DataFrame\n",
    "for name, df in dataframes:\n",
    "    # Take 50 random samples or fewer if the DataFrame has fewer rows\n",
    "    sampled_df = df\n",
    "    # Add a column with the source name\n",
    "    sampled_df[\"Source\"] = name\n",
    "    # Append to the combined DataFrame\n",
    "    combined_df = pd.concat([combined_df, sampled_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df['feature_list'] = combined_df['feature_list'].apply(\n",
    "    lambda x: \"; \".join(ast.literal_eval(x)) if isinstance(x, str) else x\n",
    ")\n",
    "combined_df['style_attributes'] = df['style_attributes'].apply(ast.literal_eval)\n",
    "combined_df.to_csv('/kaggle/working/Combined_Data.csv', index=False)\n",
    "\n",
    "print(\"Combined DataFrame saved to 'Combined_Data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:53:48.765182Z",
     "iopub.status.busy": "2025-01-04T15:53:48.764742Z",
     "iopub.status.idle": "2025-01-04T15:53:56.796756Z",
     "shell.execute_reply": "2025-01-04T15:53:56.795677Z",
     "shell.execute_reply.started": "2025-01-04T15:53:48.765144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# print(type(df['style_attributes'][0]))\n",
    "df=pd.read_csv(\"/kaggle/input/stylumia-everything/Combined_Data.csv\")\n",
    "\n",
    "def rename_product_ids(df, column_name):\n",
    "    df[column_name] = [f'product_{i}' for i in range(len(df))]\n",
    "    return df\n",
    "    \n",
    "df=df.drop(columns=[\"Source\"])\n",
    "df = rename_product_ids(df, 'product_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for k,v in df.items():\n",
    "    print(k,type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting ProductName sentence to proper ProductName using Flan-t5\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-04T15:56:00.587509Z",
     "iopub.status.busy": "2025-01-04T15:56:00.587223Z",
     "iopub.status.idle": "2025-01-04T16:01:04.785269Z",
     "shell.execute_reply": "2025-01-04T16:01:04.784320Z",
     "shell.execute_reply.started": "2025-01-04T15:56:00.587487Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 4. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 8. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 10, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1  # GPU if available, otherwise CPU\n",
    "\n",
    "# Load the summarization model and move it to GPU\n",
    "summarizer = pipeline(\"summarization\", model=\"google/flan-t5-small\", device=device)\n",
    "\n",
    "# Function to summarize product names and clean the result\n",
    "def summarize_product_name(product_name):\n",
    "    try:\n",
    "        # Summarize using the pipeline\n",
    "        result = summarizer(product_name, max_length=10, min_length=5, do_sample=False)\n",
    "        summary_text = result[0]['summary_text']\n",
    "        \n",
    "        # Remove everything before and including the first colon\n",
    "        clean_summary = summary_text.split(\":\", 1)[-1].strip()  # Split at the first colon and keep the part after it\n",
    "        \n",
    "        return clean_summary\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Apply the function to the product_name column\n",
    "df[\"product_name\"] = df[\"product_name\"].apply(summarize_product_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neo4J Ingestion\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T10:02:30.117509Z",
     "iopub.status.busy": "2025-01-05T10:02:30.117220Z",
     "iopub.status.idle": "2025-01-05T10:02:30.121290Z",
     "shell.execute_reply": "2025-01-05T10:02:30.120489Z",
     "shell.execute_reply.started": "2025-01-05T10:02:30.117487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NEO4J_URI=\"\"\n",
    "NEO4J_USERNAME=\"\"\n",
    "NEO4J_PASSWORD=\"\"\n",
    "AURA_INSTANCEID=\"\"\n",
    "AURA_INSTANCENAME=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T10:02:31.203845Z",
     "iopub.status.busy": "2025-01-05T10:02:31.203369Z",
     "iopub.status.idle": "2025-01-05T10:02:35.063220Z",
     "shell.execute_reply": "2025-01-05T10:02:35.062322Z",
     "shell.execute_reply.started": "2025-01-05T10:02:31.203806Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-e9904eab13c4>:2: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph(\n"
     ]
    }
   ],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T16:11:08.303747Z",
     "iopub.status.busy": "2025-01-04T16:11:08.303437Z",
     "iopub.status.idle": "2025-01-04T16:11:08.310784Z",
     "shell.execute_reply": "2025-01-04T16:11:08.309953Z",
     "shell.execute_reply.started": "2025-01-04T16:11:08.303716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_dataframe_and_populate_graph(df, graph):\n",
    "    \"\"\"\n",
    "    Process the dataframe to populate the Neo4j knowledge graph.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame containing product data.\n",
    "    - graph: Neo4jGraph connection instance.\n",
    "    \"\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract product metadata\n",
    "        product_id = row.get('product_id')\n",
    "        product_name = row.get('product_name')\n",
    "        description = row.get('description')\n",
    "        category_id = row.get('category_id')\n",
    "        category_name = row.get('category_name')\n",
    "        brand = row.get('brand')\n",
    "        retailer_name = row.get('Retailer_name')\n",
    "        department_id = row.get('department_id')\n",
    "        mrp = row.get('mrp')\n",
    "        pdp_url = row.get('pdp_url')\n",
    "        feature_image = row.get('feature_image')\n",
    "        feature_image_s3 = row.get('feature_image_s3')\n",
    "        channel_id = row.get('channel_id')\n",
    "        meta_info = row.get('meta_info')\n",
    "        feature_list = row.get('feature_list',[])\n",
    "        style_attributes = row.get('style_attributes')\n",
    "        \n",
    "        # Insert product node and relationships\n",
    "        product_query = \"\"\"\n",
    "            MERGE (product:Product {productName: $productName})\n",
    "            SET product += {\n",
    "                productID: $productID,\n",
    "                description: $description,\n",
    "                mrp: $mrp,\n",
    "                pdp_url: $pdp_url,\n",
    "                feature_image: $feature_image,\n",
    "                feature_image_s3: $feature_image_s3,\n",
    "                channel_id: $channel_id,\n",
    "                meta_info: $meta_info,\n",
    "                feature_list: $feature_list\n",
    "            }\n",
    "            MERGE (category:Category {categoryID: $categoryID, categoryName: $categoryName})\n",
    "            MERGE (brand:Brand {name: $brand})\n",
    "            MERGE (retailer:Retailer {name: $retailer_name})\n",
    "            MERGE (department:Department {departmentID: $departmentID})\n",
    "            MERGE (product)-[:BELONGS_TO]->(category)\n",
    "            MERGE (product)-[:BRAND]->(brand)\n",
    "            MERGE (product)-[:SOLD_BY]->(retailer)\n",
    "            MERGE (product)-[:ASSOCIATED_WITH]->(department)\n",
    "        \"\"\"\n",
    "        graph.query(\n",
    "            product_query,\n",
    "            {\n",
    "                \"productName\": product_name,\n",
    "                \"productID\": product_id,\n",
    "                \"description\": description,\n",
    "                \"mrp\": mrp,\n",
    "                \"pdp_url\": pdp_url,\n",
    "                \"feature_image\": feature_image,\n",
    "                \"feature_image_s3\": feature_image_s3,\n",
    "                \"channel_id\": channel_id,\n",
    "                \"categoryID\": category_id,\n",
    "                \"categoryName\": category_name,\n",
    "                \"brand\": brand,\n",
    "                \"retailer_name\": retailer_name,\n",
    "                \"departmentID\": department_id,\n",
    "                \"meta_info\" : meta_info,\n",
    "                \"feature_list\" : feature_list\n",
    "            },\n",
    "        )\n",
    "\n",
    "        for key, value in style_attributes.items():\n",
    "            attribute_query = \"\"\"\n",
    "                MERGE (attribute:StyleAttribute {name: $key})\n",
    "                MERGE (product:Product {productName: $productName})\n",
    "                CREATE (product)-[:HAS_ATTRIBUTE {value: $value}]->(attribute)\n",
    "            \"\"\"\n",
    "            graph.query(attribute_query, {\"productName\": product_name, \"key\": key, \"value\": value})\n",
    "    \n",
    "    print(\"Dataframe processed and knowledge graph populated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-04T19:07:58.403Z",
     "iopub.execute_input": "2025-01-04T16:11:08.449964Z",
     "iopub.status.busy": "2025-01-04T16:11:08.449716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "process_dataframe_and_populate_graph(df, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T10:02:10.229463Z",
     "iopub.status.busy": "2025-01-05T10:02:10.229130Z",
     "iopub.status.idle": "2025-01-05T10:02:10.233058Z",
     "shell.execute_reply": "2025-01-05T10:02:10.232281Z",
     "shell.execute_reply.started": "2025-01-05T10:02:10.229434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T10:02:36.937455Z",
     "iopub.status.busy": "2025-01-05T10:02:36.937159Z",
     "iopub.status.idle": "2025-01-05T10:02:37.051605Z",
     "shell.execute_reply": "2025-01-05T10:02:37.050986Z",
     "shell.execute_reply.started": "2025-01-05T10:02:36.937432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Define the Prompt for Query Generation with Context\n",
    "prompt_template = \"\"\"\n",
    "You are an expert in querying Neo4j knowledge graphs using Cypher. The knowledge graph is structured with the following schema and relationships:\n",
    "\n",
    "1. Product nodes (`:Product`) have properties:\n",
    "   - `productID`\n",
    "   - `productName`\n",
    "   - `description`\n",
    "   - `mrp`\n",
    "   - `pdp_url`\n",
    "   - `feature_image`\n",
    "   - `feature_image_s3`\n",
    "   - `channel_id`\n",
    "   - `feature_list`\n",
    "   - `meta_info`\n",
    "   \n",
    "\n",
    "2. Relationships:\n",
    "   - `[:BELONGS_TO]` connects a `Product` to a `Category` node.\n",
    "   - `[:BRAND]` connects a `Product` to a `Brand` node.\n",
    "   - `[:SOLD_BY]` connects a `Product` to a `Retailer` node.\n",
    "   - `[:ASSOCIATED_WITH]` connects a `Product` to a `Department` node.\n",
    "   - `[:HAS_ATTRIBUTE]` connects a `Product` to a `StyleAttribute` node with a `value` property on its relationship arrow.\n",
    "\n",
    "3. Category:\n",
    "   - Watches\n",
    "   - T-Shirts\n",
    "   - Kurtis\n",
    "   - Dresses\n",
    "   - Earrings\n",
    "   - Jeans\n",
    "   - Sarees\n",
    "   - Sneakers\n",
    "   - Bathroom Vanities\n",
    "   - Shirts\n",
    "\n",
    "4. Style Attribute for each categories:\n",
    "    1. Bathroom Vanities:\n",
    "       - Top Material, Vanity Features, Top Color, Assembled, Sink Color, Product Depth (in.), Faucet Included, Number of Shelves Included in Cabinet, Mirror Height (in.), Cabinet Color, Internet Sku, Cabinet Color Family, Top Color Family, Basin Depth (in.), Cabinet Shade, Top weight (lb.), Sink Color Family, Number of Sinks, Manufacturer Warranty, Vanity Top Thickness (in.), Hardware Finish Family, Style, Sink Type, Sink Material, Basin Length (in.), Mirror Width (in.), Backsplash Height (in.), Cabinet Width (in.), Cabinet & Top Assembled weight (lbs), Mirror Features, Returnable, Vanity Top Edge Type, Sink Location, Cabinet Height (in.), Product Width (in.), Included, Cabinet Material, Backsplash Included, Vanity Type, Cabinet Depth (in.), Store Sku, Sink Shape, Mirror Included, Faucet Hole Spacing (in.), Number of Drawers, Model, Basin Width (in.), Product Height (in.)\n",
    "    \n",
    "    2. Kurtis:\n",
    "       - Supplier Information\n",
    "    \n",
    "    3. Dresses:\n",
    "       - Care Instructions, Composition, Description, Sleeve Length, Size, Material, Fit, Country of Production, Price (MRP), Date of Manufacture, Length, Additional Material Information, Style, Common Generic Name, Net Quantity, Date of Import\n",
    "    \n",
    "    4. Earrings:\n",
    "       - Size & Fit, Details & Care\n",
    "    \n",
    "    5. Jeans:\n",
    "       - Composition, Color, Goods SN, Material, Waist Line, Type, Details, Fit Type, Care Instructions, Lined for Added Warmth, Belt, Length, Pattern Type, Features, Closure Type, Pockets, Fabric, Sheer\n",
    "    \n",
    "    6. Sarees:\n",
    "       - No attributes available or error occurred.\n",
    "    \n",
    "    7. Sneakers:\n",
    "       - Sole Material, Pattern, Toe Shape, Occasion, Material & Care, BIS Certificate Image URL, BIS Expiry Date, Micro Trend, Material, Insole, Type, Fastening, Seller Name, BIS Certificate Number, Shoe Width, Ankle Height, Sustainable, Number of Items, Package Contains\n",
    "    \n",
    "    8. T-Shirts:\n",
    "       - Fit, Care Instructions, Composition, Sleeve Length, Neckline, Length, Imported, Detailed Description, Model Size, Material\n",
    "    \n",
    "    9. Watches:\n",
    "       - Calendar Type, Date First Available, Part Number, Band Colour, ASIN, Dial Colour, Product Dimensions, Bezel Function, Special Features, Case Thickness, Department, Display Type, Warranty Type, Manufacturer, Net Quantity, Case Material, Importer, Crystal Material, Movement, Warranty, Item Model Number, Model Number, Generic Name, Band Size, Packer, Case Diameter, Brand, Case Shape, Bezel Material, Band Material, Clasp, Band Width, Item Dimensions (LxWxH), Country of Origin, Item Weight\n",
    "    \n",
    "    10. Shirts:\n",
    "        - Composition, Sleeve Length, Neckline, Color, Sleeve Type, Goods SN, Material, Hem Shaped, Type, Details, Fit Type, Care Instructions, Length, Pattern Type, Style, Fabric, Placket, Sheer\n",
    "    \n",
    "\n",
    "5. Examples of Cypher queries based on user input:\n",
    "   - **Find products of a specific brand (e.g., Casio):**\n",
    "     MATCH (p:Product)-[:BRAND]->(b:Brand)\n",
    "     WHERE TOLOWER(b.name) CONTAINS TOLOWER('CASIO')\n",
    "     RETURN p\n",
    "\n",
    "   - **Find products with a specific strap color (e.g., silver):**\n",
    "     MATCH (p:Product)-[r:HAS_ATTRIBUTE]->(a:StyleAttribute)\n",
    "     WHERE TOLOWER(a.name) = TOLOWER('Strap Color') AND TOLOWER(r.value) CONTAINS TOLOWER('Silver')\n",
    "     RETURN p\n",
    "\n",
    "   - **Find products in a specific category (e.g., Electronics):**\n",
    "     MATCH (p:Product)-[:BELONGS_TO]->(c:Category)\n",
    "     WHERE TOLOWER(c.categoryName) CONTAINS TOLOWER('Electronics')\n",
    "     RETURN p\n",
    "\n",
    "   - **Products similar to shein brand's pants**\n",
    "     MATCH (p:Product)-[:BRAND]->(b:Brand), (p:Product)-[:BELONGS_TO]->(c:Category)\n",
    "     WHERE TOLOWER(b.name) CONTAINS TOLOWER('shein') AND TOLOWER(c.categoryName) CONTAINS TOLOWER('Jeans')\n",
    "     RETURN p\n",
    "\n",
    "\n",
    "6. Instructions:\n",
    "   - Always use `CONTAINS` and `TOLOWER` to handle partial matches and case-insensitivity.\n",
    "   - If the user query mentions an attribute (e.g., \"strap color\"), map it to a `StyleAttribute` node.\n",
    "   - If the query includes a brand, category, retailer, or department, map it to the respective nodes and relationships.\n",
    "\n",
    "Here is the user query:\n",
    "\"{user_query}\"\n",
    "\n",
    "Generate the corresponding Cypher query based on the user's request. Respond with only the Cypher query.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Step 4: Set Up the LLM Chain\n",
    "query_generation_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Step 5: Function to Process the Query\n",
    "def process_user_query(user_query):\n",
    "    # Generate Cypher Query using the LLM\n",
    "    user_query = user_query.lower()\n",
    "    cypher_query = query_generation_chain.run(user_query=user_query)\n",
    "    print(\"Generated Cypher Query:\")\n",
    "    print(cypher_query)\n",
    "    \n",
    "    # Execute the Cypher Query on the Neo4j graph\n",
    "    try:\n",
    "        results = graph.query(cypher_query)\n",
    "        if not results:  # Check if the query returned results\n",
    "            return \"No results found for the query.\"\n",
    "        \n",
    "        # Format results dynamically\n",
    "        formatted_results = \"\"\n",
    "        for idx, record in enumerate(results, start=1):\n",
    "            formatted_results += f\"Product {idx}:\\n\"\n",
    "            \n",
    "            # Iterate over each field in the record\n",
    "            for key, value in record.items():\n",
    "                if key == 'p':  # Handle the 'p' (Product) node specifically\n",
    "                    product_data = value\n",
    "                    for product_key, product_value in product_data.items():\n",
    "                        formatted_results += f\"  {product_key.replace('_', ' ').title()}: {product_value}\\n\"\n",
    "                else:  # Include other fields like Sleeve_Type dynamically\n",
    "                    formatted_results += f\"  {key.replace('_', ' ').title()}: {value}\\n\"\n",
    "            \n",
    "            formatted_results += \"\\n\"  # Separate products with a blank line\n",
    "        \n",
    "        return formatted_results.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T10:09:36.604929Z",
     "iopub.status.busy": "2025-01-05T10:09:36.604606Z",
     "iopub.status.idle": "2025-01-05T10:09:36.610839Z",
     "shell.execute_reply": "2025-01-05T10:09:36.609889Z",
     "shell.execute_reply.started": "2025-01-05T10:09:36.604902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_user_query(user_query):\n",
    "    # Generate Cypher Query using the LLM\n",
    "    user_query = user_query.lower()\n",
    "    cypher_query = query_generation_chain.run(user_query=user_query)\n",
    "    print(\"Generated Cypher Query:\")\n",
    "    print(cypher_query)\n",
    "    \n",
    "    # Execute the Cypher Query on the Neo4j graph\n",
    "    try:\n",
    "        results = graph.query(cypher_query)\n",
    "        if not results:  # Check if the query returned results\n",
    "            return \"No results found for the query.\"\n",
    "        \n",
    "        # Format results to include only ProductID and Feature Image\n",
    "        formatted_results = \"\"\n",
    "        for idx, record in enumerate(results, start=1):\n",
    "            product_id = record.get('p', {}).get('productID', 'N/A')\n",
    "            feature_image = record.get('p', {}).get('feature_image_s3', 'N/A')\n",
    "            \n",
    "            formatted_results += f\"Product {idx}:\\n\"\n",
    "            formatted_results += f\"  ProductID: {product_id}\\n\"\n",
    "            formatted_results += f\"  Feature Image: {feature_image}\\n\\n\"\n",
    "        \n",
    "        return formatted_results.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T10:09:37.231533Z",
     "iopub.status.busy": "2025-01-05T10:09:37.231231Z",
     "iopub.status.idle": "2025-01-05T10:09:43.600202Z",
     "shell.execute_reply": "2025-01-05T10:09:43.599408Z",
     "shell.execute_reply.started": "2025-01-05T10:09:37.231511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher Query:\n",
      "MATCH (p:Product)-[r:HAS_ATTRIBUTE]->(a:StyleAttribute), (p:Product)-[:BELONGS_TO]->(c:Category)\n",
      "WHERE TOLOWER(a.name) = TOLOWER('Color') AND TOLOWER(r.value) CONTAINS TOLOWER('red') AND (TOLOWER(c.categoryName) CONTAINS TOLOWER('jeans') OR TOLOWER(c.categoryName) CONTAINS TOLOWER('shirts') OR TOLOWER(c.categoryName) CONTAINS TOLOWER('dresses') OR TOLOWER(c.categoryName) CONTAINS TOLOWER('shoes'))\n",
      "RETURN p\n",
      "Query Results:\n",
      "Product 1:\n",
      "  ProductID: 179cf1b9bf70a57159232ff3ab97c0f9cbadf9173fc1d49271fdd69ad8ebfa1a\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/11/49/179cf1b9bf70a57159232ff3ab97c0f9cbadf9173fc1d49271fdd69ad8ebfa1a.jpg\n",
      "\n",
      "Product 2:\n",
      "  ProductID: 179cf1b9bf70a57159232ff3ab97c0f9cbadf9173fc1d49271fdd69ad8ebfa1a\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/11/49/179cf1b9bf70a57159232ff3ab97c0f9cbadf9173fc1d49271fdd69ad8ebfa1a.jpg\n",
      "\n",
      "Product 3:\n",
      "  ProductID: 5c942c0273dac512fda21b7b9360db066061e2b067b33712075b2c1e58b0a244\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/09/49/5c942c0273dac512fda21b7b9360db066061e2b067b33712075b2c1e58b0a244.jpg\n",
      "\n",
      "Product 4:\n",
      "  ProductID: 76691ec1c217edcd39ae41bf17f95766504c38d668cda4a4523e97715a181297\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/11/49/76691ec1c217edcd39ae41bf17f95766504c38d668cda4a4523e97715a181297.jpg\n",
      "\n",
      "Product 5:\n",
      "  ProductID: e91e257fee6783ee98ab693f4e965de6e802fbaafc57c501df23ce26eb650f55\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/10/49/e91e257fee6783ee98ab693f4e965de6e802fbaafc57c501df23ce26eb650f55.jpg\n",
      "\n",
      "Product 6:\n",
      "  ProductID: 24c7ddb3bb973c7aab5181bdbcb5c322dff6afbbe4ab12f674159bd87d5481d5\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/11/49/24c7ddb3bb973c7aab5181bdbcb5c322dff6afbbe4ab12f674159bd87d5481d5.jpg\n",
      "\n",
      "Product 7:\n",
      "  ProductID: 53b937cc773bf4cb7dda9bda56403b80978fa339549662661355601793849d95\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/10/49/53b937cc773bf4cb7dda9bda56403b80978fa339549662661355601793849d95.jpg\n",
      "\n",
      "Product 8:\n",
      "  ProductID: 4cd7b8669942bd693d3e1762c7924459d5970dc7b42cf3af79ca80fc230c8905\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/09/49/4cd7b8669942bd693d3e1762c7924459d5970dc7b42cf3af79ca80fc230c8905.jpg\n",
      "\n",
      "Product 9:\n",
      "  ProductID: 1449e9d751d27617bb859fef3ad9dcb52c5686100d4708391e71477b50b65574\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/11/49/1449e9d751d27617bb859fef3ad9dcb52c5686100d4708391e71477b50b65574.jpg\n",
      "\n",
      "Product 10:\n",
      "  ProductID: e3a1b318c10646846b529ec63bb088093beb08c05d7f13d83e502e4ff651e59d\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/10/49/e3a1b318c10646846b529ec63bb088093beb08c05d7f13d83e502e4ff651e59d.jpg\n",
      "\n",
      "Product 11:\n",
      "  ProductID: fb0555eb75207321e78c749eff244700cb0aae304a6c4f0c29bcc53906dae5e8\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/10/49/fb0555eb75207321e78c749eff244700cb0aae304a6c4f0c29bcc53906dae5e8.jpg\n",
      "\n",
      "Product 12:\n",
      "  ProductID: f6ca2a8254a6c687de16e5dfeb4eb4eaa6d1213bec7be452654210127fccbf66\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/11/49/f6ca2a8254a6c687de16e5dfeb4eb4eaa6d1213bec7be452654210127fccbf66.jpg\n",
      "\n",
      "Product 13:\n",
      "  ProductID: b190323157f659f0a9d6ef8e4e5353cd947402f2db4d2a1b18a7266dcd758f88\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/10/49/b190323157f659f0a9d6ef8e4e5353cd947402f2db4d2a1b18a7266dcd758f88.jpg\n"
     ]
    }
   ],
   "source": [
    "user_input = \"All the product that match red colour jeans shirt dresses shoes\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T10:11:16.941248Z",
     "iopub.status.busy": "2025-01-05T10:11:16.940941Z",
     "iopub.status.idle": "2025-01-05T10:11:21.589308Z",
     "shell.execute_reply": "2025-01-05T10:11:21.588618Z",
     "shell.execute_reply.started": "2025-01-05T10:11:16.941225Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher Query:\n",
      "MATCH (p:Product)-[:BELONGS_TO]->(c:Category), (p:Product)-[r:HAS_ATTRIBUTE]->(a:StyleAttribute)\n",
      "WHERE TOLOWER(c.categoryName) CONTAINS TOLOWER('shirts') AND TOLOWER(a.name) = TOLOWER('color') AND TOLOWER(r.value) CONTAINS TOLOWER('red')\n",
      "RETURN p\n",
      "Query Results:\n",
      "Product 1:\n",
      "  ProductID: e3a1b318c10646846b529ec63bb088093beb08c05d7f13d83e502e4ff651e59d\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/10/49/e3a1b318c10646846b529ec63bb088093beb08c05d7f13d83e502e4ff651e59d.jpg\n",
      "\n",
      "Product 2:\n",
      "  ProductID: fb0555eb75207321e78c749eff244700cb0aae304a6c4f0c29bcc53906dae5e8\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/10/49/fb0555eb75207321e78c749eff244700cb0aae304a6c4f0c29bcc53906dae5e8.jpg\n",
      "\n",
      "Product 3:\n",
      "  ProductID: f6ca2a8254a6c687de16e5dfeb4eb4eaa6d1213bec7be452654210127fccbf66\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/11/49/f6ca2a8254a6c687de16e5dfeb4eb4eaa6d1213bec7be452654210127fccbf66.jpg\n",
      "\n",
      "Product 4:\n",
      "  ProductID: b190323157f659f0a9d6ef8e4e5353cd947402f2db4d2a1b18a7266dcd758f88\n",
      "  Feature Image: https://assets.stylumia.com/originals/2024/10/49/b190323157f659f0a9d6ef8e4e5353cd947402f2db4d2a1b18a7266dcd758f88.jpg\n"
     ]
    }
   ],
   "source": [
    "user_input = \"All the product that match red shirts\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T10:02:44.260329Z",
     "iopub.status.busy": "2025-01-05T10:02:44.260094Z",
     "iopub.status.idle": "2025-01-05T10:02:47.199772Z",
     "shell.execute_reply": "2025-01-05T10:02:47.198883Z",
     "shell.execute_reply.started": "2025-01-05T10:02:44.260307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher Query:\n",
      "MATCH (p:Product)-[:HAS_ATTRIBUTE]->(a:StyleAttribute)\n",
      "WITH a.name AS feature, COUNT(*) AS frequency, COLLECT(DISTINCT p) AS products\n",
      "ORDER BY frequency DESC\n",
      "LIMIT 10\n",
      "UNWIND products AS product\n",
      "MATCH (product)-[:BELONGS_TO]->(c:Category)\n",
      "RETURN feature, frequency, COLLECT(DISTINCT c.categoryName) AS categories\n",
      "Query Results:\n",
      "Product 1:\n",
      "  Feature: Type\n",
      "  Frequency: 584\n",
      "  Categories: ['Dresses', 'Earrings', 'Denim', 'Jeans', 'Sneakers', 'Watches', 'Shirts']\n",
      "\n",
      "Product 2:\n",
      "  Feature: Material\n",
      "  Frequency: 514\n",
      "  Categories: ['Dresses', 'Denim', 'Jeans', 'Sarees', 'Sneakers', 'T-Shirts', 'Shirts']\n",
      "\n",
      "Product 3:\n",
      "  Feature: Length\n",
      "  Frequency: 475\n",
      "  Categories: ['Dresses', 'Denim', 'Jeans', 'T-Shirts', 'Shirts']\n",
      "\n",
      "Product 4:\n",
      "  Feature: goods_sn\n",
      "  Frequency: 470\n",
      "  Categories: ['Dresses', 'Denim', 'Jeans', 'T-Shirts', 'Shirts']\n",
      "\n",
      "Product 5:\n",
      "  Feature: Color\n",
      "  Frequency: 470\n",
      "  Categories: ['Dresses', 'Denim', 'Jeans', 'T-Shirts', 'Shirts']\n",
      "\n",
      "Product 6:\n",
      "  Feature: Style\n",
      "  Frequency: 458\n",
      "  Categories: ['Bathroom Vanities', 'Dresses', 'Denim', 'T-Shirts', 'Shirts']\n",
      "\n",
      "Product 7:\n",
      "  Feature: Pattern Type\n",
      "  Frequency: 457\n",
      "  Categories: ['Dresses', 'Denim', 'Jeans', 'T-Shirts', 'Shirts']\n",
      "\n",
      "Product 8:\n",
      "  Feature: Fit Type\n",
      "  Frequency: 456\n",
      "  Categories: ['Dresses', 'Denim', 'Jeans', 'T-Shirts', 'Shirts']\n",
      "\n",
      "Product 9:\n",
      "  Feature: Composition\n",
      "  Frequency: 420\n",
      "  Categories: ['Dresses', 'Denim', 'Jeans', 'T-Shirts', 'Shirts']\n",
      "\n",
      "Product 10:\n",
      "  Feature: Fabric\n",
      "  Frequency: 415\n",
      "  Categories: ['Dresses', 'Denim', 'Jeans', 'T-Shirts', 'Shirts']\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Highlight the most common features among the products and show their category also\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:33:08.882484Z",
     "iopub.status.busy": "2024-12-28T14:33:08.881975Z",
     "iopub.status.idle": "2024-12-28T14:33:11.290539Z",
     "shell.execute_reply": "2024-12-28T14:33:11.289323Z",
     "shell.execute_reply.started": "2024-12-28T14:33:08.882441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = \"Suggest me some cargo which are blue in color\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T12:40:07.825591Z",
     "iopub.status.busy": "2024-12-28T12:40:07.825119Z",
     "iopub.status.idle": "2024-12-28T12:40:10.106692Z",
     "shell.execute_reply": "2024-12-28T12:40:10.105220Z",
     "shell.execute_reply.started": "2024-12-28T12:40:07.825558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = \"Show the most common brands for each category.\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T12:41:01.108160Z",
     "iopub.status.busy": "2024-12-28T12:41:01.107657Z",
     "iopub.status.idle": "2024-12-28T12:41:03.953684Z",
     "shell.execute_reply": "2024-12-28T12:41:03.952365Z",
     "shell.execute_reply.started": "2024-12-28T12:41:01.108119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = \"Analyze how brands are distributed across categories.\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T12:41:52.400525Z",
     "iopub.status.busy": "2024-12-28T12:41:52.399900Z",
     "iopub.status.idle": "2024-12-28T12:41:56.759345Z",
     "shell.execute_reply": "2024-12-28T12:41:56.758204Z",
     "shell.execute_reply.started": "2024-12-28T12:41:52.400482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = \"Identify the most frequently used materials in Jeans.\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T12:43:10.450807Z",
     "iopub.status.busy": "2024-12-28T12:43:10.450312Z",
     "iopub.status.idle": "2024-12-28T12:43:13.801418Z",
     "shell.execute_reply": "2024-12-28T12:43:13.800057Z",
     "shell.execute_reply.started": "2024-12-28T12:43:10.450771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = \"Identify the most frequently used materials in Dresses.\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:08:38.872709Z",
     "iopub.status.busy": "2024-12-28T15:08:38.872201Z",
     "iopub.status.idle": "2024-12-28T15:08:45.903968Z",
     "shell.execute_reply": "2024-12-28T15:08:45.902225Z",
     "shell.execute_reply.started": "2024-12-28T15:08:38.872672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = \"Identify the products having style attributes same in categories of jeans, tshirt, shirt, dresses, not all included fewer can be included too.\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding of Knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:14:33.768034Z",
     "iopub.status.busy": "2025-01-04T15:14:33.767681Z",
     "iopub.status.idle": "2025-01-04T15:14:36.340317Z",
     "shell.execute_reply": "2025-01-04T15:14:36.339656Z",
     "shell.execute_reply.started": "2025-01-04T15:14:33.768004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Connect to Neo4j\n",
    "neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "def fetch_product_data():\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Product)\n",
    "    RETURN p.productId AS productId, p.description AS description, p.meta_info AS meta_info, p.feature_list AS feature_list\n",
    "    \"\"\"\n",
    "    with neo4j_driver.session() as session:\n",
    "        results = session.run(query)\n",
    "        return [\n",
    "            {\n",
    "                \"productId\": record[\"productId\"],\n",
    "                \"description\": record[\"description\"],\n",
    "                \"meta_info\": record[\"meta_info\"],\n",
    "                \"feature_list\": record[\"feature_list\"],\n",
    "            }\n",
    "            for record in results\n",
    "        ]\n",
    "\n",
    "product_data = fetch_product_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:15:08.906262Z",
     "iopub.status.busy": "2025-01-04T15:15:08.905946Z",
     "iopub.status.idle": "2025-01-04T15:15:13.677067Z",
     "shell.execute_reply": "2025-01-04T15:15:13.676387Z",
     "shell.execute_reply.started": "2025-01-04T15:15:08.906237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cb81f4af304b4b8ad1f1d2b32c3187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993348ad9ceb4c45afe7aca0ad65051a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c642d882f34a2390e7b4b8d359cdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f097f5888b034121be4523625d439719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3f33d580974fa6b59f80bc5a278f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f5643eb5f64ea2a5dd321159b6f164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5070eae92db841939cc33fe66022a9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e7ee72ef304fb3b407f5078477c245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d40f116b9946949bcd245154a24329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fb38ddbb204a73a786b6866fe0dc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a624084cb2b40ddb6fa1b67ee24a640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize embeddings with the HuggingFace model on GPU\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": device}  # Load the model on the GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:15:13.678718Z",
     "iopub.status.busy": "2025-01-04T15:15:13.678163Z",
     "iopub.status.idle": "2025-01-04T15:15:15.719570Z",
     "shell.execute_reply": "2025-01-04T15:15:15.718797Z",
     "shell.execute_reply.started": "2025-01-04T15:15:13.678695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 104/104 [00:02<00:00, 51.20product/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# Generate embeddings for each product\n",
    "for product in tqdm(product_data, desc=\"Generating Embeddings\", unit=\"product\"):\n",
    "    # Safely handle non-string or missing values\n",
    "    description = str(product.get(\"description\", \"\") or \"\")\n",
    "    meta_info = str(product.get(\"meta_info\", \"\") or \"\")\n",
    "    feature_list = product.get(\"feature_list\", \"\")\n",
    "    \n",
    "    # Handle feature_list if it's a string representation of a list\n",
    "    if isinstance(feature_list, str):\n",
    "        try:\n",
    "            feature_list = \"; \".join(ast.literal_eval(feature_list))\n",
    "        except (ValueError, SyntaxError):\n",
    "            feature_list = feature_list  # Keep as is if parsing fails\n",
    "    elif isinstance(feature_list, list):\n",
    "        feature_list = \"; \".join(feature_list)\n",
    "    else:\n",
    "        feature_list = \"\"\n",
    "\n",
    "    # Combine text fields\n",
    "    combined_text = f\"{description} {meta_info} {feature_list}\"\n",
    "\n",
    "    # Generate embedding\n",
    "    product[\"embedding\"] = embeddings.embed_query(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T05:57:26.884125Z",
     "iopub.status.busy": "2025-01-05T05:57:26.883844Z",
     "iopub.status.idle": "2025-01-05T05:57:47.720101Z",
     "shell.execute_reply": "2025-01-05T05:57:47.719152Z",
     "shell.execute_reply.started": "2025-01-05T05:57:26.884104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4f6a3a04c14c158fb13eb463d415c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c78ed67db7436ba15eb729b0550008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a192e6ac5534f45b1ce6e3361468d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60452da1593b49f5a18af9fe5480e763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150215f792a9433fa552b5279c0830c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7e6cef0b9d4f9cab0f95d3191ef778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58752c7a9a6c43498d137b50f2c43b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98efcf23096a4a5d9b29d8e329f43531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ca084fe6ed4ae5a6df5fb0480a5d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f012b0eb54f40f083ec1c48140027da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5475586930cd480cb60b02b43060b30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_neo4j import Neo4jVector\n",
    "\n",
    "neo4j_graph_vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    node_label=\"Product\",\n",
    "    text_node_properties = [\n",
    "    \"feature_image\",\n",
    "    \"productID\",\n",
    "    \"meta_info\",\n",
    "    \"feature_list\",\n",
    "    \"description\",\n",
    "    \"pdp_url\",\n",
    "    \"mrp\",\n",
    "    \"channel_id\",\n",
    "    \"productName\",\n",
    "    \"feature_image_s3\"\n",
    "    ],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:15:15.913770Z",
     "iopub.status.busy": "2025-01-04T15:15:15.913541Z",
     "iopub.status.idle": "2025-01-04T15:15:38.562044Z",
     "shell.execute_reply": "2025-01-04T15:15:38.561217Z",
     "shell.execute_reply.started": "2025-01-04T15:15:15.913750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Storing Embeddings in Graph: 100%|██████████| 104/104 [00:22<00:00,  4.59product/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def store_embeddings_in_graph(product_data):\n",
    "    with neo4j_driver.session() as session:\n",
    "        for product in tqdm(product_data, desc=\"Storing Embeddings in Graph\", unit=\"product\"):\n",
    "            query = \"\"\"\n",
    "            MATCH (p:Product {productId: $productId})\n",
    "            SET p.embedding = $embedding\n",
    "            \"\"\"\n",
    "            session.run(query, productId=product[\"productId\"], embedding=product[\"embedding\"])\n",
    "\n",
    "# Call the function with progress tracking\n",
    "store_embeddings_in_graph(product_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T05:57:57.437933Z",
     "iopub.status.busy": "2025-01-05T05:57:57.437226Z",
     "iopub.status.idle": "2025-01-05T05:57:58.443315Z",
     "shell.execute_reply": "2025-01-05T05:57:58.442352Z",
     "shell.execute_reply.started": "2025-01-05T05:57:57.437902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = neo4j_graph_vector_index.similarity_search(\"Can you recommend me products similar to shein brand's top\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T05:57:58.444923Z",
     "iopub.status.busy": "2025-01-05T05:57:58.444568Z",
     "iopub.status.idle": "2025-01-05T05:57:58.449733Z",
     "shell.execute_reply": "2025-01-05T05:57:58.448650Z",
     "shell.execute_reply.started": "2025-01-05T05:57:58.444886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_image: https://adn-static1.nykaa.com/nykdesignstudio-images/pub/media/catalog/product/5/1/51bfbb2TES828_1.jpg?rnd=20200526195200\n",
      "productID: 122a67a42f0297f26d2582faf535a49e2f02de630f79c50ca0b7a200893053e9\n",
      "meta_info: Paisley Pattern Dry Clean , Do Not Machine Wash Care instructions Woven Casual Occasion Single Pack Size 1 Saree, 1 Unstitched Blouse Pack contains #917, 5th Cross Rd, Hrbr Layout 1st Block, Hrbr Layout, Banaswadi, Bengaluru, Karnataka- 560043 Address of Manufacturer/ Packer/ Importer Jw Brands Private Limited Sold By India Country of Origin Jw Brands Name of Manufacturer/ Packer/ Importer\n",
      "feature_list: []\n",
      "description: Step into grace with this Teejh Mohini Coral Banarasi Tissue Saree with Unstitched Blouse. Teejh Ethnic sarees come with a blouse piece, which can be stitched according to your taste and preference, with this beautiful design and fabric, this saree will fetch you oodles of compliments and praise.\n",
      "pdp_url: https://www.nykaafashion.com/teejh-mohini-coral-banarasi-tissue-saree-with-unstitched/p/16960960\n",
      "mrp: 10999\n",
      "channel_id: 211\n",
      "productName: Mohini Coral Banarasi Tissue Saree with Unstitched Blouse\n",
      "feature_image_s3: https://assets.stylumia.com/originals/2024/11/211/122a67a42f0297f26d2582faf535a49e2f02de630f79c50ca0b7a200893053e9.jpg\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "print(result[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making ImageCaptioning Using BLIP Using Langchain Similarity\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from io import BytesIO\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": device}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to generate a caption from an image URL using BLIP-2\n",
    "def generate_image_caption(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch or process image from URL: {image_url} | Error: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        caption = model.generate(**inputs)\n",
    "    return processor.decode(caption[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Connect to Neo4j\n",
    "neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "def fetch_product_data():\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Product)\n",
    "    RETURN p.productID AS productID, p.description AS description, p.meta_info AS meta_info, p.feature_list AS feature_list, p.feature_image_s3 AS feature_image_s3\n",
    "    \"\"\"\n",
    "    with neo4j_driver.session() as session:\n",
    "        results = session.run(query)\n",
    "        return [\n",
    "            {\n",
    "                \"productID\": record[\"productID\"],\n",
    "                \"description\": record[\"description\"],\n",
    "                \"meta_info\": record[\"meta_info\"],\n",
    "                \"feature_list\": record[\"feature_list\"],\n",
    "                \"feature_image_s3\": record[\"feature_image_s3\"]\n",
    "            }\n",
    "            for record in results\n",
    "        ]\n",
    "\n",
    "# Function to embed a product using weighted text\n",
    "def generate_product_embedding(product):\n",
    "    weighted_text = generate_weighted_text(product)\n",
    "    return embeddings.embed_query(weighted_text)\n",
    "product_data = fetch_product_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for product in tqdm(product_data, desc=\"Processing Products\"):\n",
    "    # Compute the embedding for the product\n",
    "    embedding = generate_product_embedding(product)\n",
    "    # Store the embedding in the product dictionary\n",
    "    product[\"embedding\"] = embedding\n",
    "\n",
    "# Confirm embeddings are stored\n",
    "print(\"Embeddings computed and stored in product_data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "neo4j_graph_vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    node_label=\"Product\",\n",
    "    text_node_properties=[\n",
    "        \"feature_image\",\n",
    "        \"productID\",\n",
    "        \"meta_info\",\n",
    "        \"feature_list\",\n",
    "        \"description\",\n",
    "        \"pdp_url\",\n",
    "        \"mrp\",\n",
    "        \"channel_id\",\n",
    "        \"productName\",\n",
    "        \"feature_image_s3\",\n",
    "    ],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def store_embeddings_in_graph(product_data):\n",
    "    with neo4j_driver.session() as session:\n",
    "\n",
    "        print(\"Storing new embeddings...\")\n",
    "        for product in tqdm(product_data, desc=\"Storing Embeddings in Graph\", unit=\"product\"):\n",
    "\n",
    "            # Store the embedding in Neo4j\n",
    "            query = \"\"\"\n",
    "            MATCH (p:Product {productID: $productID})\n",
    "            SET p.embedding = $embedding\n",
    "            \"\"\"\n",
    "            session.run(query, productID=product[\"productID\"], embedding=product[\"embedding\"])\n",
    "\n",
    "# Call the function\n",
    "store_embeddings_in_graph(product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = neo4j_graph_vector_index.similarity_search(\"Give me products of shein tshirts\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Query Uing Hybrid Search (Vector + Embedding ) \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NEO4J_USERNAME=\"\"\n",
    "NEO4J_PASSWORD=\"\"\n",
    "AURA_INSTANCEID=\"\"\n",
    "AURA_INSTANCENAME=\"\"\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_neo4j import Neo4jVector\n",
    "from tqdm import tqdm\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Define the Prompt for Query Generation with Context\n",
    "prompt_template = \"\"\"\n",
    "You are an expert in querying Neo4j knowledge graphs using Cypher. The knowledge graph is structured with the following schema and relationships:\n",
    "\n",
    "1. Product nodes (`:Product`) have properties:\n",
    "   - `productID`\n",
    "   - `productName`\n",
    "   - `description`\n",
    "   - `mrp`\n",
    "   - `pdp_url`\n",
    "   - `feature_image`\n",
    "   - `feature_image_s3`\n",
    "   - `channel_id`\n",
    "   - `feature_list`\n",
    "   - `meta_info`\n",
    "   \n",
    "\n",
    "2. Relationships:\n",
    "   - `[:BELONGS_TO]` connects a `Product` to a `Category` node.\n",
    "   - `[:BRAND]` connects a `Product` to a `Brand` node.\n",
    "   - `[:SOLD_BY]` connects a `Product` to a `Retailer` node.\n",
    "   - `[:ASSOCIATED_WITH]` connects a `Product` to a `Department` node.\n",
    "   - `[:HAS_ATTRIBUTE]` connects a `Product` to a `StyleAttribute` node with a `value` property on its relationship arrow.\n",
    "\n",
    "3. Category:\n",
    "   - Watches\n",
    "   - T-Shirts\n",
    "   - Kurtis\n",
    "   - Dresses\n",
    "   - Earrings\n",
    "   - Jeans\n",
    "   - Sarees\n",
    "   - Sneakers\n",
    "   - Bathroom Vanities\n",
    "   - Shirts\n",
    "\n",
    "4. Style Attribute for each categories:\n",
    "    1. Bathroom Vanities:\n",
    "       - Top Material, Vanity Features, Top Color, Assembled, Sink Color, Product Depth (in.), Faucet Included, Number of Shelves Included in Cabinet, Mirror Height (in.), Cabinet Color, Internet Sku, Cabinet Color Family, Top Color Family, Basin Depth (in.), Cabinet Shade, Top weight (lb.), Sink Color Family, Number of Sinks, Manufacturer Warranty, Vanity Top Thickness (in.), Hardware Finish Family, Style, Sink Type, Sink Material, Basin Length (in.), Mirror Width (in.), Backsplash Height (in.), Cabinet Width (in.), Cabinet & Top Assembled weight (lbs), Mirror Features, Returnable, Vanity Top Edge Type, Sink Location, Cabinet Height (in.), Product Width (in.), Included, Cabinet Material, Backsplash Included, Vanity Type, Cabinet Depth (in.), Store Sku, Sink Shape, Mirror Included, Faucet Hole Spacing (in.), Number of Drawers, Model, Basin Width (in.), Product Height (in.)\n",
    "    \n",
    "    2. Kurtis:\n",
    "       - Supplier Information\n",
    "    \n",
    "    3. Dresses:\n",
    "       - Care Instructions, Composition, Description, Sleeve Length, Size, Material, Fit, Country of Production, Price (MRP), Date of Manufacture, Length, Additional Material Information, Style, Common Generic Name, Net Quantity, Date of Import\n",
    "    \n",
    "    4. Earrings:\n",
    "       - Size & Fit, Details & Care\n",
    "    \n",
    "    5. Jeans:\n",
    "       - Composition, Color, Goods SN, Material, Waist Line, Type, Details, Fit Type, Care Instructions, Lined for Added Warmth, Belt, Length, Pattern Type, Features, Closure Type, Pockets, Fabric, Sheer\n",
    "    \n",
    "    6. Sarees:\n",
    "       - No attributes available or error occurred.\n",
    "    \n",
    "    7. Sneakers:\n",
    "       - Sole Material, Pattern, Toe Shape, Occasion, Material & Care, BIS Certificate Image URL, BIS Expiry Date, Micro Trend, Material, Insole, Type, Fastening, Seller Name, BIS Certificate Number, Shoe Width, Ankle Height, Sustainable, Number of Items, Package Contains\n",
    "    \n",
    "    8. T-Shirts:\n",
    "       - Fit, Care Instructions, Composition, Sleeve Length, Neckline, Length, Imported, Detailed Description, Model Size, Material\n",
    "    \n",
    "    9. Watches:\n",
    "       - Calendar Type, Date First Available, Part Number, Band Colour, ASIN, Dial Colour, Product Dimensions, Bezel Function, Special Features, Case Thickness, Department, Display Type, Warranty Type, Manufacturer, Net Quantity, Case Material, Importer, Crystal Material, Movement, Warranty, Item Model Number, Model Number, Generic Name, Band Size, Packer, Case Diameter, Brand, Case Shape, Bezel Material, Band Material, Clasp, Band Width, Item Dimensions (LxWxH), Country of Origin, Item Weight\n",
    "    \n",
    "    10. Shirts:\n",
    "        - Composition, Sleeve Length, Neckline, Color, Sleeve Type, Goods SN, Material, Hem Shaped, Type, Details, Fit Type, Care Instructions, Length, Pattern Type, Style, Fabric, Placket, Sheer\n",
    "    \n",
    "\n",
    "5. Examples of Cypher queries based on user input:\n",
    "   - **Find products of a specific brand (e.g., Casio):**\n",
    "     MATCH (p:Product)-[:BRAND]->(b:Brand)\n",
    "     WHERE TOLOWER(b.name) CONTAINS TOLOWER('CASIO')\n",
    "     RETURN p\n",
    "\n",
    "   - **Find products with a specific strap color (e.g., silver):**\n",
    "     MATCH (p:Product)-[r:HAS_ATTRIBUTE]->(a:StyleAttribute)\n",
    "     WHERE TOLOWER(a.name) = TOLOWER('Strap Color') AND TOLOWER(r.value) CONTAINS TOLOWER('Silver')\n",
    "     RETURN p\n",
    "\n",
    "   - **Find products in a specific category (e.g., Electronics):**\n",
    "     MATCH (p:Product)-[:BELONGS_TO]->(c:Category)\n",
    "     WHERE TOLOWER(c.categoryName) CONTAINS TOLOWER('Electronics')\n",
    "     RETURN p\n",
    "\n",
    "   - **Products similar to shein brand's pants**\n",
    "     MATCH (p:Product)-[:BRAND]->(b:Brand), (p:Product)-[:BELONGS_TO]->(c:Category)\n",
    "     WHERE TOLOWER(b.name) CONTAINS TOLOWER('shein') AND TOLOWER(c.categoryName) CONTAINS TOLOWER('Jeans')\n",
    "     RETURN p\n",
    "\n",
    "\n",
    "6. Instructions:\n",
    "   - Always use `CONTAINS` and `TOLOWER` to handle partial matches and case-insensitivity.\n",
    "   - If the user query mentions an attribute (e.g., \"strap color\"), map it to a `StyleAttribute` node.\n",
    "   - If the query includes a brand, category, retailer, or department, map it to the respective nodes and relationships.\n",
    "\n",
    "Here is the user query:\n",
    "\"{user_query}\"\n",
    "\n",
    "Generate the corresponding Cypher query based on the user's request. Respond with only the Cypher query.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Step 4: Set Up the LLM Chain\n",
    "query_generation_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def process_similarity_results(similarity_results):\n",
    "    # Initialize a string to store the formatted results\n",
    "    formatted_results = \"\"\n",
    "    \n",
    "    # Loop through each document in the similarity results\n",
    "    for idx, result in enumerate(similarity_results, start=1):\n",
    "        page_content = result.page_content  # Access the page_content directly\n",
    "        \n",
    "        # Split the page_content into lines and format each line accordingly\n",
    "        formatted_results += f\"Product {idx}:\\n\"\n",
    "        \n",
    "        # Split the page_content by lines\n",
    "        lines = page_content.strip().split('\\n')\n",
    "        \n",
    "        # Process each line and format key-value pairs\n",
    "        for line in lines:\n",
    "            # Split each line by the first occurrence of \":\"\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip().replace('_', ' ').title()  # Clean and format the key\n",
    "                value = value.strip()  # Clean up the value\n",
    "                \n",
    "                # Handle the case where the description is NaN or empty\n",
    "                if value.lower() == 'nan' or not value:\n",
    "                    value = \"No data available\"\n",
    "                \n",
    "                formatted_results += f\"  {key}: {value}\\n\"\n",
    "        \n",
    "        # Add a blank line to separate products\n",
    "        formatted_results += \"\\n\"\n",
    "    \n",
    "    # Return the formatted string\n",
    "    return formatted_results.strip()\n",
    "\n",
    "\n",
    "# Step 5: Function to Process the Query\n",
    "def process_user_query(user_query):\n",
    "    # Generate Cypher Query using the LLM\n",
    "    user_query = user_query.lower()\n",
    "    cypher_query = query_generation_chain.run(user_query=user_query)\n",
    "    print(\"Generated Cypher Query:\")\n",
    "    print(cypher_query)\n",
    "    \n",
    "    # Execute the Cypher Query on the Neo4j graph\n",
    "    try:\n",
    "        results = graph.query(cypher_query)\n",
    "        if not results:  # Check if the query returned results\n",
    "            print(\"No cypher query results can be generated! using vector semantic search\")\n",
    "            similarity_results = neo4j_graph_vector_index.similarity_search(user_query, k=5)\n",
    "            # Format results if no Cypher query results are found\n",
    "            return process_similarity_results(similarity_results)\n",
    "        \n",
    "        # Format results dynamically\n",
    "        formatted_results = \"\"\n",
    "        for idx, record in enumerate(results, start=1):\n",
    "            formatted_results += f\"Product {idx}:\\n\"\n",
    "            \n",
    "            # Iterate over each field in the record\n",
    "            for key, value in record.items():\n",
    "                if key == 'p':  # Handle the 'p' (Product) node specifically\n",
    "                    product_data = value\n",
    "                    for product_key, product_value in product_data.items():\n",
    "                        formatted_results += f\"  {product_key.replace('_', ' ').title()}: {product_value}\\n\"\n",
    "                else:  # Include other fields like Sleeve_Type dynamically\n",
    "                    formatted_results += f\"  {key.replace('_', ' ').title()}: {value}\\n\"\n",
    "            \n",
    "            formatted_results += \"\\n\"  # Separate products with a blank line\n",
    "        \n",
    "        return formatted_results.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_input = \"Can you recommend me sweater\"\n",
    "results = process_user_query(user_input)\n",
    "print(\"Query Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6380421,
     "sourceId": 10307275,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6421346,
     "sourceId": 10367392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
